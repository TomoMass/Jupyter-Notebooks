{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anaconda3下でのInstall\n",
    "\n",
    "condaではtensorboard実行に問題が出たので一旦これで。\n",
    "\n",
    "```\n",
    "$ pip install -i https://pypi.anaconda.org/jjhelmus/simple tensorflow\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hi, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello = tf.constant('Hi, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlowを簡単な計算で理解する\n",
    "\n",
    "> [TensorFlowを算数で理解する - Qiita](http://qiita.com/icoxfog417/items/fb5c24e35a849f8e2c5d)\n",
    "\n",
    "### スカラ量で計算してみる\n",
    "\n",
    "- 計算する数式\n",
    "$$ \n",
    "y = x^2 + b\n",
    "$$\n",
    "\n",
    "\n",
    "- flowを図示すると\n",
    "\n",
    "```\n",
    "0.5 --> x--[square]--[add]--> result\n",
    "                      |\n",
    "3.0 --> b-------------+\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.25]\n"
     ]
    }
   ],
   "source": [
    "def x2_plus_b(x, b):\n",
    "    _x = tf.constant(x)\n",
    "    _b = tf.constant(b)\n",
    "    result = tf.square(_x)\n",
    "    result = tf.add(result, _b)\n",
    "    return result\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run([x2_plus_b(.5,3.)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoardによる可視化\n",
    "\n",
    "- 実行後に、コマンドラインから下記を実行\n",
    " \n",
    " ```\n",
    "$ tensorboard --logdir=/path/to/log-directory\n",
    " ```\n",
    "\n",
    "- `http://0.0.0.0:6006`にアクセスする\n",
    "\n",
    "- `GRAPH`に実行済フローが複数表示される\n",
    "\n",
    "> ポイントとしては、`tf.scalar_summary`で計算した値をsummaryしておく点です。こうして計算したsummaryを、`tf.train.SummaryWriter`で書き出していきます。 http://qiita.com/icoxfog417/items/fb5c24e35a849f8e2c5d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def monitor_calculation(x, b):\n",
    "    title = \"b = {0}\".format(b)\n",
    "    c = x2_plus_b(float(x), float(b))\n",
    "    s = tf.scalar_summary(title, c)\n",
    "    m = tf.merge_summary([s])  # if you are using some summaries, merge them\n",
    "    return m\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.train.SummaryWriter(\"log\", graph_def=sess.graph_def)    \n",
    "    xaxis = range(-10, 12)\n",
    "\n",
    "    for b in range(3):\n",
    "        for x in xaxis:\n",
    "            summary_str = sess.run(monitor_calculation(x, b))\n",
    "            writer.add_summary(summary_str, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スカラ量で計算する\n",
    "\n",
    "> https://www.tensorflow.org/versions/master/get_started/index.html\n",
    "\n",
    "上記コードはPython2ベースのため、`xrange()` -> `range()`に修正。\n",
    "\n",
    "> 参考 http://stackoverflow.com/questions/17192158/nameerror-global-name-xrange-is-not-defined-in-python-3\n",
    "\n",
    "- やっていることは回帰分析\n",
    "\n",
    "> 「y = 0.1 x + 0.3」という数式を使って生成したxとyを学習データとして、y = W x + b という数式のWとbを最適化していく http://qiita.com/MATS_ELB/items/dfc50149d52e47e5a07b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.07575795] [ 0.5411855]\n",
      "20 [ 0.02578249] [ 0.33941314]\n",
      "40 [ 0.07675866] [ 0.31234229]\n",
      "60 [ 0.09272193] [ 0.30386502]\n",
      "80 [ 0.09772088] [ 0.30121034]\n",
      "100 [ 0.09928628] [ 0.30037904]\n",
      "120 [ 0.0997765] [ 0.30011871]\n",
      "140 [ 0.09993] [ 0.30003718]\n",
      "160 [ 0.09997807] [ 0.30001166]\n",
      "180 [ 0.09999313] [ 0.30000365]\n",
      "200 [ 0.09999785] [ 0.30000114]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "\n",
    "# Try to find values for W and b that compute y_data = W * x_data + b\n",
    "# (We know that W should be 0.1 and b 0.3, but Tensorflow will\n",
    "# figure that out for us.)\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = W * x_data + b\n",
    "\n",
    "# Minimize the mean squared errors.\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# Before starting, initialize the variables.  We will 'run' this first.\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph.\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Fit the line.\n",
    "for step in range(201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(W), sess.run(b))\n",
    "\n",
    "# Learns best fit is W: [0.1], b: [0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テンソルで計算する\n",
    "\n",
    "> http://qiita.com/MATS_ELB/items/fec7f54de2dd18b043ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 0.12049839  0.63580847]\n",
      " [ 0.7111249  -0.02813603]] [ 0.05483134  0.09086838]\n",
      "100 [[ 0.15628858  0.04232659]\n",
      " [ 0.05392477  0.14043736]] [ 0.27025187  0.27766153]\n",
      "200 [[ 0.10838222  0.00629473]\n",
      " [ 0.00802105  0.10602347]] [ 0.29557258  0.29667521]\n",
      "300 [[ 0.10124753  0.00093688]\n",
      " [ 0.00119379  0.10089649]] [ 0.29934105  0.29950514]\n",
      "400 [[ 0.10018568  0.00013946]\n",
      " [ 0.0001777   0.10013343]] [ 0.29990193  0.29992634]\n",
      "500 [[  1.00027628e-01   2.07803114e-05]\n",
      " [  2.64537048e-05   1.00019872e-01]] [ 0.29998541  0.29998901]\n",
      "600 [[  1.00004129e-01   3.12404495e-06]\n",
      " [  3.96192627e-06   1.00002974e-01]] [ 0.29999781  0.29999834]\n",
      "700 [[  1.00000627e-01   5.01336388e-07]\n",
      " [  6.14700070e-07   1.00000471e-01]] [ 0.29999965  0.29999974]\n",
      "800 [[  1.00000247e-01   2.22809277e-07]\n",
      " [  2.17059082e-07   1.00000232e-01]] [ 0.29999986  0.29999986]\n",
      "900 [[  1.00000247e-01   2.22755844e-07]\n",
      " [  2.14493255e-07   1.00000232e-01]] [ 0.29999986  0.29999986]\n",
      "1000 [[  1.00000247e-01   2.22767198e-07]\n",
      " [  2.14479655e-07   1.00000232e-01]] [ 0.29999986  0.29999986]\n"
     ]
    }
   ],
   "source": [
    "# tfという名前で参照できるようにtensorflowを、npという名前で参照できるようにインポート\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# y_data = W_data * x_data + b_data というx_dataとy_dataの関係になるようにあらかじめ\n",
    "# W_dataとb_dataの値を定める。機械学習が適切に行われるとWはこのW_dataに、bはこのb_dataに近づく\n",
    "# なお、W_dataは２行２列のテンソル、b_dataは２行１列のテンソル\n",
    "W_data = np.array([[0.1, 0], [0, 0.1]])\n",
    "b_data = np.array([0.3, 0.3])\n",
    "\n",
    "# 乱数生成を利用して0から1の間の数値を持つ２行１列の行列「X_data」を浮動小数として100個生成\n",
    "x_data = np.random.rand(100, 2, 1).astype(\"float32\")\n",
    "\n",
    "# その後で、生成した100個のxに対して、行列版でのy=0.1x+0.3となるようなyを100個生成\n",
    "y_data = W_data * x_data + b_data\n",
    "\n",
    "# 上記で生成したxとy（共に２行１列のテンソル）の組を学習データとして用いる\n",
    "\n",
    "# 機械学習で最適化するWとbを設定する。Wは２行２列のテンソル。bは２行１列のテンソル。\n",
    "W = tf.Variable(tf.random_uniform([2, 2], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "y = W * x_data + b\n",
    "\n",
    "# 学習において、その時点での学習のダメ程度を表すlossを、学習データのyとその時点でのyの差の２乗と定義\n",
    "# Wとbの最適化のアルゴリズムを最急降下法（勾配法）とし、その１回の最適化処理にoptimizerと名前を付ける\n",
    "# 上記の最適化処理の繰り返しによりlossを最小化する処理をtrainと呼ぶことにする\n",
    "loss = tf.reduce_mean(tf.square(y_data - y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 学習を始める前にこのプログラムで使っている変数を全てリセットして空っぽにする\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph.（おきまりの文句）\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 学習を1000回行い、100回目ごとに画面に学習回数とWとbのその時点の値を表示する\n",
    "for step in range(1001):\n",
    "    sess.run(train)\n",
    "    if step % 100 == 0:\n",
    "        print(step, sess.run(W), sess.run(b))\n",
    "\n",
    "# Learns best fit is W: [[0.1, 0], [0, 0.1]], b: [0.3, 0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 次は同内容のTensorboard可視化（エラーあり）\n",
    "\n",
    "> http://qiita.com/MATS_ELB/items/fec7f54de2dd18b043ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tfという名前で参照できるようにtensorflowを、npという名前で参照できるようにインポート\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# y_data = W_data * x_data + b_data というx_dataとy_dataの関係になるようにあらかじめ\n",
    "# W_dataとb_dataの値を定める。機械学習が適切に行われるとWはこのW_dataに、bはこのb_dataに近づく\n",
    "# なお、W_dataは２行２列のテンソル、b_dataは２行１列のテンソル\n",
    "W_data = np.array([[0.1, 0], [0, 0.1]])\n",
    "b_data = np.array([0.3, 0.3])\n",
    "\n",
    "# 乱数生成を利用して0から1の間の数値を持つ２行１列の行列「X_data」を浮動小数として100個生成\n",
    "x_data = np.random.rand(100, 2, 1).astype(\"float32\")\n",
    "\n",
    "# その後で、生成した100個のxに対して、行列版でのy=0.1x+0.3となるようなyを100個生成\n",
    "y_data = W_data * x_data + b_data\n",
    "\n",
    "# 上記で生成したxとy（共に２行１列のテンソル）の組を学習データとして用いる\n",
    "\n",
    "# 機械学習で最適化するWとbを設定する。Wは２行２列のテンソル。bは２行１列のテンソル。\n",
    "W = tf.Variable(tf.random_uniform([2, 2], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "y = W * x_data + b\n",
    "\n",
    "## TensorBoardへ表示するための変数を用意する（ヒストグラム用）\n",
    "W_hist = tf.histogram_summary(\"weights\", W)\n",
    "b_hist = tf.histogram_summary(\"biases\", b)\n",
    "y_hist = tf.histogram_summary(\"y\", y)\n",
    "\n",
    "# 学習において、その時点での学習のダメ程度を表すlossを、学習データのyとその時点でのyの差の２乗と定義\n",
    "# Wとbの最適化のアルゴリズムを最急降下法（勾配法）とし、その１回の最適化処理にoptimizerと名前を付ける\n",
    "# 上記の最適化処理の繰り返しによりlossを最小化する処理をtrainと呼ぶことにする\n",
    "loss = tf.reduce_mean(tf.square(y_data - y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "## TensorBoardへloss（学習のダメ具合の指標として設定したスカラー値）を表示するための変数を用意する（イベント用）\n",
    "loss_sum = tf.scalar_summary(\"loss\", loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# 上記で用意した合計４つのTensorBoard描画用の変数を、TensorBoardが利用するSummaryデータとしてmerge（合体）する\n",
    "# また、そのSummaryデータを書き込むSummaryWriterを用意し、書き込み先を'/tmp/tf_logs'ディレクトリに指定する\n",
    "merged = tf.merge_all_summaries()\n",
    "writer = tf.train.SummaryWriter(\"/tmp/tf_logs\", sess.graph_def)\n",
    "\n",
    "\n",
    "# 学習を始める前にこのプログラムで使っている変数を全てリセットして空っぽにする\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph.（おきまりの文句）\n",
    "sess.run(init)\n",
    "\n",
    "# 学習を1000回行い、10回目ごとにSummaryデータを保存し、画面にWとbのその時点の値を表示する\n",
    "for step in range(1001):\n",
    "    if step % 10 == 0:\n",
    "        result = sess.run([merged, loss])\n",
    "        summary_str = result[0]\n",
    "        writer.add_summary(summary_str, step)\n",
    "        print(step, sess.run(W), sess.run(b))\n",
    "    else:\n",
    "        sess.run(train)\n",
    "\n",
    "# Learns best fit is W: [[0.1, 0], [0, 0.1]], b: [0.3, 0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TenorFlow Tutorials\n",
    "\n",
    "### MNIST for ML beginners (91%)\n",
    "\n",
    "> [TensorFlow::MNIST for ML beginners](https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html#mnist-for-ml-beginners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0.9166\n"
     ]
    }
   ],
   "source": [
    "# Implementing the Regression\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# Training\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_:batch_ys})\n",
    "\n",
    "# Evaluating Our Model\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep MNIST for Experts (99%)\n",
    "\n",
    "> https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts\n",
    "\n",
    "- 翻訳\n",
    "\n",
    "> http://qiita.com/KojiOhki/items/64a2ee54214b01a411c7\n",
    "\n",
    "- ソースは下記より(macbook pro early2015 - 22 sec /100 steps -> 73 min / 20000 steps)\n",
    "\n",
    "> http://qiita.com/haminiku/items/36982ae65a770565458d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# mnistデータ読み込み\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "##########################################\n",
    "# 深層畳み込みニューラルネットワークを構築\n",
    "# Build a Multilayer Convolutional Network\n",
    "# 精度91%は悪いから深層畳み込みモデルを構築して99.2%を目指す\n",
    "###########################################\n",
    "\n",
    "\"\"\"\n",
    "ちょっと理解できませんでした.. \n",
    "多層になると損失関数のパラメータ勾配が限りなくゼロに近づく勾配消失問題(Vanishing gradient problem)対策のために、少量のノイズで重みを初期化する関数みたいです。\n",
    "\n",
    "Weight Initialization\n",
    "\n",
    "To create this model, we're going to need to create a lot of weights and biases.\n",
    "One should generally initialize weights with a small amount of noise for symmetry breaking,\n",
    "and to prevent 0 gradients. Since we're using ReLU neurons, it is also good practice to initialize\n",
    "them with a slightly positive initial bias to avoid \"dead neurons.\" Instead of doing this repeatedly\n",
    "while we build the model, let's create two handy functions to do it for us.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\"\"\"\n",
    "Convolution and Pooling\n",
    "TensorFlow also gives us a lot of flexibility in convolution and pooling operations.\n",
    "How do we handle the boundaries? What is our stride size? In this example,\n",
    "we're always going to choose the vanilla version. Our convolutions uses a stride of one\n",
    "and are zero padded so that the output is the same size as the input. Our pooling is plain old\n",
    "max pooling over 2x2 blocks. To keep our code cleaner, let's also abstract those operations into functions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\"\"\"\n",
    "第1レイヤー 5x5パッチで32の特徴を計算\n",
    "[5, 5, 1, 32] は最初の5,5はパッチサイズ,1は入力チャンネル数,32は出力チャンネル数\n",
    "\"\"\"\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\"\"\"\n",
    "第2レイヤー 5x5パッチで64の特徴を計算\n",
    "\"\"\"\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\"\"\"\n",
    "密集接続レイヤー\n",
    "\n",
    "画像サイズ7x7に還元されているので、1024のニューロンと完全に接続する層（翻訳がかなり怪しいので原文読んでください）MNISTデータは28x28ピクセルなので1/16ずつ読むみたいです。\n",
    "\n",
    "Densely Connected Layer\n",
    "\n",
    "Now that the image size has been reduced to 7x7, we add a fully-connected layer with 1024 neurons to allow\n",
    "processing on the entire image. We reshape the tensor from the pooling layer into a batch of vectors,\n",
    "multiply by a weight matrix, add a bias, and apply a ReLU.\n",
    "\"\"\"\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\"\"\"\n",
    "過剰適合を排除する\n",
    "\n",
    "Dropout\n",
    "\n",
    "To reduce overfitting, we will apply dropout before the readout layer. We create a placeholder\n",
    "for the probability that a neuron's output is kept during dropout. This allows us to turn dropout\n",
    "on during training, and turn it off during testing. TensorFlow's tf.nn.dropout op automatically\n",
    "handles scaling neuron outputs in addition to masking them, so dropout just works without any additional scaling.\n",
    "\"\"\"\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\"\"\"\n",
    "読み出し層\n",
    "第1層のロジスティック回帰のように、ロジスティック回帰層を追加\n",
    "\n",
    "Readout Layer\n",
    "Finally, we add a softmax layer, just like for the one layer softmax regression above.\n",
    "\"\"\"\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "\"\"\"\n",
    "モデルの学習と評価\n",
    "TensorFlowを使用して、洗練された深い学習モデルの学習と評価を行います。\n",
    "\"\"\"\n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "# 結果表示\n",
    "print(\"test accuracy %g\" % accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow公式レポジトリを使う\n",
    "\n",
    "### git cloneする\n",
    "\n",
    "```\n",
    "$ git clone https://github.com/tensorflow/tensorflow\n",
    "```\n",
    "\n",
    "`/tensorflow/tensorflow/examples`にtutorials等のsource codeあり。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
